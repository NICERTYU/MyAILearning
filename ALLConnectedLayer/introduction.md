在我看来，全连接网络是一个矩阵和非线性激活的组合。


在我们的线性代数上我们都学过矩阵乘法比如，![img](https://file+.vscode-resource.vscode-cdn.net/Users/gw/Desktop/MyAILearning/MyAILearning/ALLConnectedLayer/%E6%88%AA%E5%B1%8F2024-08-28%2021.07.37.png "矩阵乘法示意图")

在这个简单的例子中，我们发现了一个有趣的性质，左边矩阵的每一个样本的特征数变为了2（原来是3）。

而在全连接网络中，一个样本的特征由n为变为m维也只是进行了矩阵乘法。然而在全连接层中还有非线性激活，也就是对矩阵中的每一个数进行一个函数变换。当然每一行的同一个的特征的计算应用同一个变换函数。这是由于对每一个重新计算的特征值的计算利用了右边同一列，当然我们也可以把他称呼为神经元。


好了，来一个总结，全连接网络，这是利用了矩阵乘法和函数变换的一个普通层。接下来我们实战一下，只使用全连接层进行神经网络的搭建，进行mnist数字识别和函数拟合。


补充一下，全连接网络也被称为MLP(多层感知机)
